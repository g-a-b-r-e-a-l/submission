{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a16cdc",
   "metadata": {},
   "source": [
    "# Ploting predictions along a solvent sequence\n",
    "\n",
    "The code in this section was used to plot the predictions of the model for the 'Template' and 'Scratch' predictions against the true values, showing the range and standard devation of predictions as well as the standard devidation of each property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76482c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict_properties/test_predictions.json', 'r') as res_file:\n",
    "        results_dict = json.load(res_file)\n",
    "\n",
    "with open('data/normalisation_stats.json', 'r') as norm_file:\n",
    "        norm_dict = json.load(norm_file)\n",
    "print(norm_dict)\n",
    "base_values = pd.read_csv('data/test_values.csv')\n",
    "\n",
    "std_vals = {}\n",
    "print(norm_dict)\n",
    "for key, vals in norm_dict.items():\n",
    "        print(vals)\n",
    "        std_vals[key] = vals['std']\n",
    "\n",
    "with open('predict_properties/template_preds.json', 'r') as template_file:\n",
    "        temp_dict = json.load(template_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_predictions(dictionary):\n",
    "    predictions_averaged = {}\n",
    "\n",
    "    for smiles, properties in dictionary.items():\n",
    "        predictions_averaged[smiles] = {}\n",
    "\n",
    "        for prop, indices in properties.items():\n",
    "            total_sum = 0\n",
    "            total_count = 0\n",
    "            \n",
    "            # Calculate the sum and count of all predictions\n",
    "            for index, prediction_list in indices.items():\n",
    "                total_sum += sum(prediction_list)\n",
    "                total_count += len(prediction_list)\n",
    "            \n",
    "            # Calculate the average, handling the case where total_count is zero to prevent division by zero\n",
    "            if total_count > 0:\n",
    "                average_prediction = total_sum / total_count\n",
    "            else:\n",
    "                average_prediction = 0 # or None, depending on desired behavior\n",
    "            \n",
    "            predictions_averaged[smiles][prop] = average_prediction\n",
    "    return predictions_averaged\n",
    "    \n",
    "predictions_averaged = avg_predictions(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values_dict = {}\n",
    "\n",
    "# Use df.itertuples() for a memory-efficient way to iterate over DataFrame rows.\n",
    "# 'index=False' prevents the row index from being included in the tuple.\n",
    "for row in base_values.itertuples(index=False):\n",
    "    # The first element of the tuple is the SMILES string\n",
    "    smiles = row[1]\n",
    "    \n",
    "    # Initialize a new dictionary for this SMILES if it doesn't exist\n",
    "    if smiles not in true_values_dict:\n",
    "        true_values_dict[smiles] = {}\n",
    "        \n",
    "    # Iterate through the rest of the columns to get the properties and values\n",
    "    # We slice the row tuple from the second element (index 1) onwards.\n",
    "    # We also get the corresponding column names from df.columns, excluding 'SMILES'.\n",
    "    for prop_name, value in zip(base_values.columns[1:], row[1:]):\n",
    "        # Store the value in the nested dictionary\n",
    "        true_values_dict[smiles][prop_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8354d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_data(property_data):\n",
    "    \"\"\"Helper function to extract and sort plotting data from a dictionary.\"\"\"\n",
    "    n_values, means, std_devs, mins, maxs = [], [], [], [], []\n",
    "    # Sort keys numerically to ensure the plot is drawn in the correct order\n",
    "    sorted_ns = sorted(map(int, property_data.keys()))\n",
    "    for n in sorted_ns:\n",
    "        n_str = str(n)\n",
    "        values = property_data[n_str]\n",
    "        n_values.append(n)\n",
    "        means.append(np.mean(values))\n",
    "        std_devs.append(np.std(values))\n",
    "        mins.append(min(values))\n",
    "        maxs.append(max(values))\n",
    "    return n_values, means, std_devs, mins, maxs\n",
    "\n",
    "def compare_predictions_by_n(dict_1, dict_2, ground_truth=None, ground_truth_std=None,\n",
    "                             label1='Dataset 1', label2='Dataset 2', output_dir='plots'):\n",
    "    \"\"\"\n",
    "    Create plots comparing predictions for each property from two different datasets.\n",
    "    \n",
    "    Generates a side-by-side view:\n",
    "    1. A plot of the predictions (with std dev) against the ground truth value.\n",
    "    2. A zoomed-in plot of the mean residuals (prediction - ground truth).\n",
    "    \n",
    "    Args:\n",
    "        dict_1 (dict): The first dictionary containing the data.\n",
    "        dict_2 (dict): The second dictionary containing the data.\n",
    "        ground_truth (dict): Dictionary mapping property names to their ground truth values.\n",
    "        ground_truth_std (dict): Dictionary mapping property names to the ground truth standard deviation.\n",
    "        label1 (str): Label for the first dataset.\n",
    "        label2 (str): Label for the second dataset.\n",
    "        output_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    data1 = dict_1\n",
    "    data2 = dict_2\n",
    "    plt.rcParams['font.size'] = 18\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_properties = set(data1.keys()) | set(data2.keys())\n",
    "\n",
    "    for property_name in all_properties:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 8), sharex=True)\n",
    "\n",
    "        all_n_values = []\n",
    "        gt_value = ground_truth.get(property_name) if ground_truth else None\n",
    "\n",
    "        # --- Process and Plot Dataset 1 ---\n",
    "        if property_name in data1:\n",
    "            n_values1, means1, std_devs1, mins1, maxs1 = _process_data(data1[property_name])\n",
    "            all_n_values.extend(n_values1)\n",
    "            \n",
    "            # Plot predictions WITH standard deviation on the first axis (ax1)\n",
    "            ax1.errorbar(n_values1, means1, yerr=std_devs1, fmt='-o',\n",
    "                         capsize=5, capthick=2, label=f'{label1} Mean ± Std Dev',\n",
    "                         color=\"#dacb00\", alpha=0.8)\n",
    "            ax1.fill_between(n_values1, mins1, maxs1, alpha=0.4,\n",
    "                             label=f'{label1} Min/Max Range', color='#dacb00')\n",
    "\n",
    "            # If ground truth exists, plot residuals WITHOUT standard deviation on the second axis (ax2)\n",
    "            if gt_value is not None:\n",
    "                residuals_mean1 = np.array(means1) - gt_value\n",
    "                ax2.plot(n_values1, residuals_mean1, '-o',\n",
    "                         label=f'{label1} Mean Residual',\n",
    "                         color=\"#dacb00\", alpha=0.8)\n",
    "\n",
    "        # --- Process and Plot Dataset 2 ---\n",
    "        if property_name in data2:\n",
    "            n_values2, means2, std_devs2, mins2, maxs2 = _process_data(data2[property_name])\n",
    "            all_n_values.extend(n_values2)\n",
    "\n",
    "            # Plot predictions WITH standard deviation on the first axis (ax1)\n",
    "            ax1.errorbar(n_values2, means2, yerr=std_devs2, fmt='-s',\n",
    "                         capsize=5, capthick=2, label=f'{label2} Mean ± Std Dev',\n",
    "                         color=\"#010055\", alpha=0.8)\n",
    "            ax1.fill_between(n_values2, mins2, maxs2, alpha=0.4,\n",
    "                             label=f'{label2} Min/Max Range', color='#010055')\n",
    "\n",
    "            # If ground truth exists, plot residuals WITHOUT standard deviation on the second axis (ax2)\n",
    "            if gt_value is not None:\n",
    "                residuals_mean2 = np.array(means2) - gt_value\n",
    "                ax2.plot(n_values2, residuals_mean2, '-s',\n",
    "                         label=f'{label2} Mean Residual',\n",
    "                         color=\"#010055\", alpha=0.8)\n",
    "\n",
    "        # --- Plot Ground Truth Lines and Shading ---\n",
    "        if gt_value is not None:\n",
    "            # Plot GT line on the predictions plot (ax1)\n",
    "            ax1.axhline(gt_value, linestyle='--', color=\"#019480\", linewidth=3,\n",
    "                        label=f'Ground Truth ({gt_value:.3f})', alpha=0.8)\n",
    "            \n",
    "            # Plot zero-error line on the residuals plot (ax2)\n",
    "            ax2.axhline(0, linestyle='--', color=\"#019480\", linewidth=3,\n",
    "                        label='Zero Error', alpha=0.8)\n",
    "\n",
    "            # Add GT uncertainty band ONLY to the first plot\n",
    "            if ground_truth_std and property_name in ground_truth_std:\n",
    "                gt_std = ground_truth_std[property_name]\n",
    "                ax1.axhspan(gt_value - gt_std, gt_value + gt_std, color=\"#019480\", alpha=0.1,\n",
    "                            label=f'Ground Truth ± Std ({gt_std:.3f})')\n",
    "\n",
    "        # --- Customize and Finalize Plot ---\n",
    "        fig.suptitle(f'Prediction Comparison for {property_name}', fontsize=25)\n",
    "\n",
    "        # Axis 1: Predictions\n",
    "        ax1.set_title('Predictions vs. Ground Truth')\n",
    "        ax1.set_ylabel('Prediction Value')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax1.legend()\n",
    "\n",
    "        # Axis 2: Residuals\n",
    "        ax2.set_title('Residuals (Prediction - Ground Truth)')\n",
    "        ax2.set_ylabel('Residual Value')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax2.legend()\n",
    "\n",
    "        # Shared X-axis settings\n",
    "        unique_n_values = sorted(set(all_n_values))\n",
    "        if unique_n_values:\n",
    "            ax1.set_xticks(unique_n_values)\n",
    "            ax1.set_xlabel('Prediction Position (n)')\n",
    "            ax2.set_xlabel('Prediction Position (n)')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "\n",
    "        plot_filename = os.path.join(output_dir, f'{property_name}_comparison.pdf')\n",
    "        plt.savefig(plot_filename, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f'Saved comparison plot for {property_name} to {plot_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_dict_with_lists(data_dict, stats):\n",
    "    \"\"\"\n",
    "    Unnormalizes the values in a nested dictionary using z-score statistics.\n",
    "    Handles single values and lists of values.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): The nested dictionary with normalized values.\n",
    "                          Format: {smiles: {property: value or [values]}}\n",
    "        stats (dict): The dictionary containing mean and std for each property.\n",
    "                      Format: {property: {'mean': value, 'std': value}}\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with unnormalized values.\n",
    "    \"\"\"\n",
    "    unnormalized_data = copy.deepcopy(data_dict)\n",
    "    \n",
    "    for smiles, properties in unnormalized_data.items():\n",
    "        for prop, indicies in properties.items():\n",
    "            for index, values in indicies.items():\n",
    "                try:\n",
    "                    mean = stats[prop]['mean']\n",
    "                    std = stats[prop]['std']\n",
    "                    # Check if the value is a list and iterate if so\n",
    "                    if isinstance(values, list):\n",
    "                        unnormalized_data[smiles][prop][index] = [\n",
    "                            (item * std) + mean for item in values\n",
    "                        ]\n",
    "                    \n",
    "                    else:\n",
    "                        # Apply the reverse z-score formula for a single value\n",
    "                        unnormalized_value = (values * std) + mean\n",
    "                        unnormalized_data[smiles][prop][index] = unnormalized_value\n",
    "\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: Statistics not found for property '{prop}'. Skipping.\")\n",
    "                    # The original value (single or list) is kept\n",
    "                    unnormalized_data[smiles][prop] = values\n",
    "\n",
    "    return unnormalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4deed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_dict(data_dict, stats):\n",
    "    \"\"\"\n",
    "    Unnormalizes the values in a nested dictionary using z-score statistics.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): The nested dictionary with normalized values.\n",
    "                          Format: {smiles: {property: value}}\n",
    "        stats (dict): The dictionary containing mean and std for each property.\n",
    "                      Format: {property: {'mean': value, 'std': value}}\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with unnormalized values.\n",
    "    \"\"\"\n",
    "    unnormalized_data = copy.deepcopy(data_dict)\n",
    "    \n",
    "    # Iterate through each smiles string in the dictionary\n",
    "    for smiles, properties in unnormalized_data.items():\n",
    "        # Iterate through each property and its normalized value\n",
    "        for prop, value in properties.items():\n",
    "            try:\n",
    "                mean = stats[prop]['mean']\n",
    "                std = stats[prop]['std']\n",
    "                \n",
    "                # Apply the reverse z-score formula: x = (z * std) + mean\n",
    "                unnormalized_value = (value * std) + mean\n",
    "                \n",
    "                # Update the value in the new dictionary\n",
    "                unnormalized_data[smiles][prop] = unnormalized_value\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Statistics not found for property '{prop}'. Skipping unnormalization for this property.\")\n",
    "                # If stats are not found, we keep the original value\n",
    "                unnormalized_data[smiles][prop] = value\n",
    "            \n",
    "    return unnormalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_avg_unnorm = unnormalize_dict(predictions_averaged, norm_dict)\n",
    "true_values_dict_unorm = unnormalize_dict(true_values_dict, norm_dict)\n",
    "print(true_values_dict_unorm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3885f",
   "metadata": {},
   "source": [
    "## Edit 'compare predictions' to change the colours, and plot parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e403e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_unorm = unnormalize_dict_with_lists(temp_dict, norm_dict)\n",
    "results_unnorm = unnormalize_dict_with_lists(results_dict, norm_dict) \n",
    "\n",
    "compare_predictions_by_n(temp_unorm['CC(=O)c1ccccc1'],results_unnorm['CC(=O)c1ccccc1'],\n",
    "\n",
    "                      ground_truth=true_values_dict_unorm['CC(=O)c1ccccc1'], ground_truth_std=std_vals, label1='template predictions', label2='scratch predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5afa73",
   "metadata": {},
   "source": [
    "# Word Embedding Plots\n",
    "Plot word embeddings, label and circle selected points for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decoder import MultiModalRegressionTransformer\n",
    "\n",
    "model = MultiModalRegressionTransformer(384, 26, 64, 28, 6, 16, 5, 0)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "model_path = 'val_loss0.1074_DPR_0.1_MP_0.3_DM_64_TL_5_heads_16.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "embedding_layer = model.embeddings_module.property_embedding\n",
    "\n",
    "# Get the weights (embeddings)\n",
    "# .weight returns a torch.nn.Parameter, so we typically convert it to a tensor\n",
    "embeddings = embedding_layer.weight.data\n",
    "transposed_data = embeddings.T  # Transpose the embeddings tensor \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")\n",
    "print(f\"First 5 embedding vectors:\\n {embeddings[:5]}\")\n",
    "\n",
    "WORD_TOKENS = ['alkane', 'aromatic', 'halohydrocarbon', 'ether', 'ketone', 'ester', 'nitrile', 'amine', 'amide', 'misc_N_compound', 'carboxylic_acid', 'monohydric_alcohol' , 'polyhydric_alcohol', 'other','ET30', 'alpha', 'beta', 'pi_star', 'SA', 'SB', 'SP', 'SdP', 'N_mol_cm3', 'n', 'fn', 'delta']\n",
    "\n",
    "\n",
    "# Convert to NumPy array\n",
    "transposed = embeddings.T\n",
    "numpy_array = embeddings.numpy()\n",
    "\n",
    "# Create DataFrame\n",
    "# The columns will be indexed 0, 1, 2...\n",
    "df = pd.DataFrame(transposed)\n",
    "print(df.shape)\n",
    "\n",
    "df.columns = WORD_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2226c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import cm\n",
    "import re # Import the regular expressions module\n",
    "\n",
    "def pca_scatter_plot(df, n_components=2, highlight_groups=None, group_labels=None, misc_labels=None):\n",
    "    \"\"\"\n",
    "    Reduce each column (384 dimensions) to n_components using PCA and create scatter plot.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with shape (384, 26) - each column will be reduced to 2D\n",
    "        n_components: Number of PCA components (default=2)\n",
    "        highlight_groups: list of lists of column names, each group will be circled in a different color\n",
    "        group_labels: list of names (strings), one per group, used to label each circle\n",
    "        misc_labels: list of column names that will get labels but no circles (for illustration)\n",
    "    \"\"\"\n",
    "    # Transpose so each row represents a column (26 rows, 384 features each)\n",
    "    df_transposed = df.T\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Latin Modern Sans', 'DejaVu Sans', 'Arial']    \n",
    "    plt.rcParams['font.size'] = 20\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_transposed)\n",
    "    \n",
    "    # Apply PCA to reduce from 384 dimensions to 2\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.scatter(pca_result[:, 0], pca_result[:, 1], color=\"#7F51FF\", alpha=0.7, s=600)\n",
    "    \n",
    "    # Collect all labels that should be shown\n",
    "    labels_to_show = set()\n",
    "    \n",
    "    # Add circled group labels\n",
    "    if highlight_groups:\n",
    "        for group in highlight_groups:\n",
    "            labels_to_show.update(group)\n",
    "    \n",
    "    # Add misc labels\n",
    "    if misc_labels:\n",
    "        labels_to_show.update(misc_labels)\n",
    "    \n",
    "    # Add labels only for selected points (using column names)\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        if col_name in labels_to_show:\n",
    "            ax.annotate(col_name, (pca_result[i, 0], pca_result[i, 1]), \n",
    "                         xytext=(5, 5), textcoords='offset points', \n",
    "                         fontsize=15, fontweight='bold', alpha=0.8)\n",
    "    \n",
    "    # Highlight groups of points with enclosing circles\n",
    "    if highlight_groups:\n",
    "        # Use colormap\n",
    "        colormap = plt.cm.Dark2\n",
    "        \n",
    "        # Create a range of values from 0 to 1, one for each group\n",
    "        num_groups = len(highlight_groups)\n",
    "        color_values = np.linspace(0, 1, num_groups)\n",
    "\n",
    "        for g_idx, group in enumerate(highlight_groups):\n",
    "            coords = []\n",
    "            for label in group:\n",
    "                if label in df.columns:\n",
    "                    idx = list(df.columns).index(label)\n",
    "                    x, y = pca_result[idx, 0], pca_result[idx, 1]\n",
    "                    coords.append((x, y))\n",
    "            if coords:\n",
    "                coords = np.array(coords)\n",
    "                centroid = coords.mean(axis=0)\n",
    "                max_dist = np.sqrt(((coords - centroid) ** 2).sum(axis=1)).mean()\n",
    "                radius = max_dist * 2.2\n",
    "                \n",
    "                # Use the colormap function to get the color for the current group\n",
    "                color = colormap(color_values[g_idx] * 0.6)\n",
    "                \n",
    "                circle = plt.Circle(\n",
    "                    centroid, radius,\n",
    "                    color=color,\n",
    "                    fill=False, linewidth=3, linestyle='--'\n",
    "                )\n",
    "                ax.add_patch(circle)\n",
    "\n",
    "                # Add a label for the circle (use group_labels if provided, else default)\n",
    "                if group_labels and g_idx < len(group_labels):\n",
    "                    circle_label = group_labels[g_idx]\n",
    "                else:\n",
    "                    circle_label = f\"Group {g_idx+1}\"\n",
    "\n",
    "                # New code to position the label at the top of the circle\n",
    "                label_x = centroid[0]\n",
    "                label_y = centroid[1] + radius\n",
    "                \n",
    "                ax.annotate(\n",
    "                    circle_label,\n",
    "                    xy=(label_x, label_y),\n",
    "                    ha='center', va='bottom',  # Align the text's bottom to the xy point\n",
    "                    fontsize=18, fontweight='bold',\n",
    "                    color=color\n",
    "                )\n",
    "    \n",
    "\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    \n",
    "    ax.grid(False, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    name = 'word_token_embeddings'\n",
    "    plt.savefig(f'{name}.pdf', bbox_inches='tight')\n",
    "    plt.show() \n",
    "    \n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    return pca_result, pca\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "highlight_labels = [['n', 'fn', 'SP'], ['ET30', 'SA', 'alpha']]\n",
    "circle_names = ['Polarisability', 'Polarity']  # <-- names for each circle\n",
    "misc_labels = ['alkane', 'beta', 'other', 'SDP', 'carboxylic_acid', 'amine', 'delta', 'aromatic']  # <-- labels without circles\n",
    "pca_result, pca_model = pca_scatter_plot(df, n_components=2, highlight_groups=highlight_labels, group_labels=circle_names, misc_labels=misc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee07df",
   "metadata": {},
   "source": [
    "# Attention Plots \n",
    "Plot attention scores of different solvent sequences to compare pre-trained and finetuned attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention_comparison(attention_weights_dict, layers_to_compare=None, sample_idx=0, head_idx=0,\n",
    "                            token_type_ids=None, save_path=None, figsize=(16, 6)):\n",
    "    \"\"\"\n",
    "    Plot attention weights from multiple layers side by side with one shared colorbar.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.rcParams['font.size'] = 16\n",
    "\n",
    "    if layers_to_compare is None:\n",
    "        layers_to_compare = sorted(attention_weights_dict.keys())\n",
    "    \n",
    "    num_layers = len(layers_to_compare)\n",
    "    if num_layers == 0:\n",
    "        print(\"No valid layers to plot.\")\n",
    "        return\n",
    "    \n",
    "    # Add extra width for colorbar space\n",
    "    fig_width = figsize[0] + 2\n",
    "    fig, axes = plt.subplots(1, num_layers, figsize=(fig_width, figsize[1]), constrained_layout=True)\n",
    "\n",
    "    if num_layers == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    heatmaps = []\n",
    "    count = 0\n",
    "    for i, layer_name in enumerate(layers_to_compare):\n",
    "        if layer_name not in attention_weights_dict:\n",
    "            print(f\"Layer {layer_name} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        attention_weights = attention_weights_dict[layer_name]\n",
    "        attn_matrix = attention_weights[sample_idx, head_idx]\n",
    "\n",
    "        # Create heatmap WITHOUT its own colorbar\n",
    "        colour_bar = False\n",
    "        if count == 2:\n",
    "            hm = sns.heatmap(attn_matrix, \n",
    "                            cmap='viridis', \n",
    "                            cbar=True,\n",
    "                            square=True,\n",
    "                            ax=axes[i], \n",
    "                            cbar_kws={'label': 'Attention Weight'})\n",
    "        else:\n",
    "            hm = sns.heatmap(attn_matrix, \n",
    "                            cmap='viridis', \n",
    "                            cbar=False,\n",
    "                            square=True,\n",
    "                            ax=axes[i])\n",
    "        heatmaps.append(hm)\n",
    "\n",
    "        seq_len = attn_matrix.shape[0]\n",
    "\n",
    "        # Ticks every 4 positions\n",
    "        axes[i].xaxis.set_major_locator(MultipleLocator(4))\n",
    "        axes[i].yaxis.set_major_locator(MultipleLocator(4))\n",
    "\n",
    "        # Explicitly set ticks to 0..N-1 every 4\n",
    "        axes[i].set_xticks(np.arange(0, seq_len, 4))\n",
    "        axes[i].set_yticks(np.arange(0, seq_len, 4))\n",
    "        axes[i].set_xticklabels(np.arange(0, seq_len, 4))\n",
    "        axes[i].set_yticklabels(np.arange(0, seq_len, 4))\n",
    "\n",
    "        # Axis labels\n",
    "        axes[i].set_xlabel('Key Position')\n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel('Query Position')\n",
    "\n",
    "        # Replace underscores with spaces in title\n",
    "        pretty_name = layer_name.replace('_', ' ').title()\n",
    "        axes[i].set_title(pretty_name)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    # Add ONE colorbar on the right\n",
    "       # Add ONE colorbar on the right\n",
    "    '''if heatmaps:\n",
    "        cbar = fig.colorbar(heatmaps[0].collections[0], ax=axes, location=\"right\", pad=0, shrink=0.9)\n",
    "        cbar.set_label('Attention Weight')\n",
    "    '''\n",
    "\n",
    "    # Add a figure title with some whitespace\n",
    "    plt.suptitle(f'Fine-tuned Model Attention Scores', fontsize=24, y=1.08)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Comparison plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c13036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures.plot_attention import MultiModalRegressionTransformerWithWeights\n",
    "from model.decoder import MultiModalRegressionTransformer\n",
    "import torch\n",
    "original_model = MultiModalRegressionTransformer(384, 26, 64, 28, 6, 16, 5, 0)\n",
    "original_model.load_state_dict(torch.load('val_loss0.1074_DPR_0.1_MP_0.3_DM_64_TL_5_heads_16.pth'))\n",
    "\n",
    "FP_model = MultiModalRegressionTransformer(384, 26, 64, 28, 6, 16, 5, 0)\n",
    "FP_model.load_state_dict(torch.load('FP_model.pt'))\n",
    "\n",
    "#MultiModalRegressionTransformerWithWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures.plot_attention import create_modified_model_from_original, save_attention_weights\n",
    "modified_model = create_modified_model_from_original(original_model, 'all')\n",
    "fp_mod = create_modified_model_from_original(FP_model, 'all')\n",
    "from model.dataset import load_dataset\n",
    "from model.config import COLUMN_DICT, MAX_SEQUENCE_LENGTH, TOKEN_TYPE_VOCAB\n",
    "from model.collate import create_collate_fn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989fd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/train_set.csv'\n",
    "\n",
    "dataset, chemberta_dimension = load_dataset(data_path, COLUMN_DICT, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Create collate function\n",
    "configured_collate_fn = create_collate_fn(TOKEN_TYPE_VOCAB, 0)\n",
    "    \n",
    "    # Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,  # Usually no shuffling for inference\n",
    "    collate_fn=configured_collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, batch in enumerate(dataloader):\n",
    "    if index == 10:\n",
    "        sample_input = batch\n",
    "sample_input['token_type_vocab'] = TOKEN_TYPE_VOCAB\n",
    "print(sample_input.keys())\n",
    "\n",
    "attention_weights = save_attention_weights(modified_model, sample_input)\n",
    "fp_weight = save_attention_weights(fp_mod, sample_input)\n",
    "head = 3\n",
    "plot_attention_comparison(attention_weights, layers_to_compare=['layer_0', 'layer_1', 'layer_4'], head_idx=head)\n",
    "plot_attention_comparison(fp_weight, layers_to_compare=['layer_0', 'layer_1', 'layer_4'], head_idx=head)\n",
    "\n",
    "'''plot_attention_comparison(attention_weights, fp_weight, \n",
    "                              title1=\"Pretrained Model\", \n",
    "                              title2=\"Fine-tuned Model\", \n",
    "                              layers_to_compare=['layer_0', 'layer_1', 'layer_4'], \n",
    "                              sample_idx=0, \n",
    "                              head_idx=3,\n",
    "                              save_path=None, \n",
    "                              figsize=(16, 12))'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
